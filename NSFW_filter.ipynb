{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vgtu25Ik6NE2",
        "uWfhXb0Y6Tn3",
        "hHCUd0tk6eQp",
        "nxAeOrV2gmh7",
        "lyRsPouQyxUc",
        "IddZNwjEiOAl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n",
        "Install packages then execute the script, you wiill see censored images in the `training_samples/nsfw_recognized` folder"
      ],
      "metadata": {
        "id": "JL59r_D8g_04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "62vqtxRjg_U8",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf7c935-ffbd-4234-89af-7056c4d2fbf1"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/index.html"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3YF9ZXFCPBh",
        "outputId": "4dd19a8b-19e9-413a-e6e1-ed1e95726c45"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/index.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyvips-binary pyvips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWb7HGXqTSOf",
        "outputId": "11b0b6de-2fb1-422a-bfad-eb689c3544ae"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvips-binary in /usr/local/lib/python3.11/dist-packages (8.16.0)\n",
            "Requirement already satisfied: pyvips in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyvips-binary) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->pyvips-binary) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter NSFW images\n",
        "\n",
        "Execute the script to see censored images in the `training_samples/nsfw_recognized` folder.<br>\n",
        "You can empty the output folder later"
      ],
      "metadata": {
        "id": "p3Wzakf0brkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vgtu25Ik6NE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import moondream as md\n",
        "import torch"
      ],
      "metadata": {
        "id": "gX52MRTgl-xJ"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common variables"
      ],
      "metadata": {
        "id": "uWfhXb0Y6Tn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IOU_THRESHOLD        = 0.3\n",
        "CONFIDENCE_THRESHOLD = 0.1\n",
        "# Key Name: cosmic-bear-616\n",
        "MOONDREAM_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXlfaWQiOiJkZDkyM2U2Mi1iZmE4LTRhMzUtYmFmYS02MjM4NmQ0ZTAwNTIiLCJpYXQiOjE3Mzg5MDA3MTF9.TzMA1xBoGpUthKdU8tQuIrfOpAvOR_SMIB2Sb_oUVcs\"\n",
        "\n",
        "drive_prefix = \"drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "yolo_pretrained_pt = drive_prefix + \"pretrains/yolo11m.pt\"\n",
        "nsfw_pretrained_pt = drive_prefix + \"pretrains/erax_nsfw_yolo11m.pt\"\n",
        "# nsfw_pretrained_pt = drive_prefix + \"erax_nsfw_yolo11n.pt\"\n",
        "# nsfw_pretrained_pt = drive_prefix + \"erax_nsfw_yolo11s.pt\"\n",
        "visionx_pretrained_pt = drive_prefix + \"pretrains/visionx.pt\"\n",
        "\n",
        "source_path = drive_prefix + \"training_samples/selected\"\n",
        "source_path_single_not_sex = drive_prefix + \"training_samples/selected/1698797738754547.jpg\"\n",
        "source_2_paths = [\n",
        "    drive_prefix + \"training_samples/selected/1722985818194150.png\",\n",
        "    drive_prefix + \"training_samples/selected/1724897142625564.jpg\"\n",
        "]\n",
        "source_4_paths = [\n",
        "    drive_prefix + \"training_samples/selected/1698797738754547.jpg\",\n",
        "    drive_prefix + \"training_samples/selected/1722985818194150.png\",\n",
        "    drive_prefix + \"training_samples/selected/1724579240484244.jpg\",\n",
        "    drive_prefix + \"training_samples/selected/1721751848855669.jpg\"\n",
        "]\n",
        "recognized_path = drive_prefix + \"training_samples/nsfw_recognized/\""
      ],
      "metadata": {
        "id": "yfaMNQ4vrh2Q"
      },
      "execution_count": 475,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NSFW image filtering script"
      ],
      "metadata": {
        "id": "hHCUd0tk6eQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p-6zYFVhYBUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e166e644-aead-4690-8144-d728195d2c30",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1378155513893.jpg: 640x640 (no detections), 1500.8ms\n",
            "image 2/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1651102464329.png: 640x384 (no detections), 907.8ms\n",
            "image 3/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1698797738754547.jpg: 448x640 (no detections), 984.0ms\n",
            "image 4/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1701979302809986.jpg: 576x640 1 penis, 1310.2ms\n",
            "image 5/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1707539361936264.jpg: 640x448 (no detections), 1017.8ms\n",
            "image 6/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1709881907307959.jpg: 640x512 (no detections), 1184.1ms\n",
            "image 7/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1713423666380556.jpg: 640x448 (no detections), 1078.2ms\n",
            "image 8/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1718904273290232.png: 416x640 1 vagina, 1476.4ms\n",
            "image 9/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1719782258238061.jpg: 640x448 (no detections), 1748.5ms\n",
            "image 10/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1720141642525774.jpg: 640x480 (no detections), 1484.1ms\n",
            "image 11/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1720645555420435.jpg: 640x512 (no detections), 1146.0ms\n",
            "image 12/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721501914109498.jpg: 448x640 (no detections), 1002.5ms\n",
            "image 13/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721668560245611.jpg: 640x512 (no detections), 1157.9ms\n",
            "image 14/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721728036419286.jpg: 640x640 (no detections), 1508.8ms\n",
            "image 15/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721751848855669.jpg: 640x480 1 make_love, 2 nipples, 1 penis, 1113.5ms\n",
            "image 16/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721834681148922.jpg: 640x480 1 anus, 1 vagina, 1065.5ms\n",
            "image 17/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721985313295768.jpg: 640x448 2 nipples, 1 penis, 996.3ms\n",
            "image 18/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1722985818194150.png: 640x448 1 nipple, 1 vagina, 1029.8ms\n",
            "image 19/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723211294727707.jpg: 640x448 (no detections), 1818.0ms\n",
            "image 20/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723399110043132.jpg: 640x640 1 anus, 1 vagina, 2243.2ms\n",
            "image 21/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723519466751102.jpg: 384x640 1 make_love, 1 nipple, 3 peniss, 1075.9ms\n",
            "image 22/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723652428084736.jpg: 448x640 (no detections), 997.1ms\n",
            "image 23/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723903495615178.jpg: 640x544 (no detections), 1171.5ms\n",
            "image 24/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724419840794985.jpg: 448x640 (no detections), 975.9ms\n",
            "image 25/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724554844212123.jpg: 640x384 1 nipple, 1 vagina, 886.4ms\n",
            "image 26/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724579240484244.jpg: 448x640 2 nipples, 1014.3ms\n",
            "image 27/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724642194903744.png: 480x640 (no detections), 1116.4ms\n",
            "image 28/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724642947452792.png: 448x640 1 make_love, 1 nipple, 969.4ms\n",
            "image 29/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724649057907120.png: 640x480 (no detections), 1082.5ms\n",
            "image 30/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724712668295029.png: 480x640 1 make_love, 7 nipples, 1222.1ms\n",
            "image 31/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724723747204379.jpg: 640x480 1 anus, 1 make_love, 2 nipples, 1 vagina, 1696.5ms\n",
            "image 32/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724809972093412.jpg: 480x640 (no detections), 1664.3ms\n",
            "image 33/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724897142625564.jpg: 640x512 4 nipples, 2 vaginas, 1579.7ms\n",
            "image 34/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724905864533025.jpg: 640x640 1 anus, 1565.0ms\n",
            "image 35/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1725093323532977s.jpg: 640x320 1 vagina, 749.4ms\n",
            "image 36/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726352613898890.jpg: 640x640 (no detections), 1501.7ms\n",
            "image 37/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726477586651921.jpg: 640x544 1 nipple, 1289.1ms\n",
            "image 38/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726933948682880.jpg: 640x448 (no detections), 1037.8ms\n",
            "image 39/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726963008207714.jpg: 640x640 1 nipple, 1467.6ms\n",
            "image 40/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727142126122073.jpg: 608x640 (no detections), 1383.9ms\n",
            "image 41/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727200895869697.jpg: 448x640 (no detections), 1510.0ms\n",
            "image 42/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727225350044664.png: 640x480 (no detections), 1773.3ms\n",
            "image 43/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727448034643355.jpg: 640x448 (no detections), 1675.5ms\n",
            "image 44/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727971007198851.jpg: 640x448 2 nipples, 1 vagina, 1126.4ms\n",
            "image 45/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727979723469241.jpg: 640x480 2 nipples, 1078.0ms\n",
            "image 46/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728061152737184.png: 448x640 1 nipple, 969.8ms\n",
            "image 47/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728127236413072.jpg: 480x640 (no detections), 1040.6ms\n",
            "image 48/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728399968479679.jpg: 320x640 (no detections), 725.0ms\n",
            "image 49/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728480134770843.png: 640x544 2 nipples, 1 vagina, 1271.1ms\n",
            "image 50/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728557079613995.jpg: 640x384 (no detections), 960.2ms\n",
            "image 51/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729021368547796.png: 384x640 (no detections), 856.5ms\n",
            "image 52/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729345051888520.jpg: 640x384 (no detections), 867.8ms\n",
            "image 53/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729424628775413.jpg: 640x480 1 anus, 2 nipples, 1 vagina, 1300.6ms\n",
            "image 54/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729606389801732.jpg: 640x448 (no detections), 1617.5ms\n",
            "image 55/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729868006558863.jpg: 640x512 1 anus, 1692.0ms\n",
            "Speed: 4.5ms preprocess, 1248.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        }
      ],
      "source": [
        "def filter_nsfw(pretrained, source):\n",
        "    model = YOLO(pretrained)\n",
        "    results = model(source, conf=CONFIDENCE_THRESHOLD, iou=IOU_THRESHOLD)\n",
        "    json_sink = sv.JSONSink(recognized_path + \"report.json\")\n",
        "\n",
        "    # Save JSON\n",
        "    with json_sink as sink:\n",
        "        for result in results:\n",
        "            filename = os.path.basename(result.path)\n",
        "            recognized_filename = recognized_path + filename\n",
        "            annotated_image = result.orig_img.copy()\n",
        "            detected = []\n",
        "\n",
        "            h, w = annotated_image.shape[:2]\n",
        "            anchor = h if h > w else w\n",
        "\n",
        "            detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "            # Draw labels\n",
        "            # label_annotator = sv.LabelAnnotator(\n",
        "            #     text_color = sv.Color.BLACK,\n",
        "            #     text_position = sv.Position.CENTER,\n",
        "            #     text_scale = anchor/1700\n",
        "            # )\n",
        "\n",
        "            # Pixelate image\n",
        "            pixelate_annotator = sv.PixelateAnnotator(pixel_size = anchor/50)\n",
        "            annotated_image = pixelate_annotator.annotate(\n",
        "                scene = annotated_image.copy(),\n",
        "                detections = detections\n",
        "            )\n",
        "\n",
        "            # Add labels to image\n",
        "            # annotated_image = label_annotator.annotate(\n",
        "            #     annotated_image,\n",
        "            #     detections = detections\n",
        "            # )\n",
        "\n",
        "            for found in detections.data.values():\n",
        "                detected = str(found)\n",
        "                detected_count = len(found)\n",
        "\n",
        "            sink.append(\n",
        "                detections,\n",
        "                custom_data = {\n",
        "                    \"filename\": filename,\n",
        "                    \"detections\": {\n",
        "                        \"count\": detected_count,\n",
        "                        \"items\": detected\n",
        "                    }\n",
        "                }\n",
        "            )\n",
        "            pilimg = sv.cv2_to_pillow(annotated_image)\n",
        "            pilimg.save(recognized_filename)\n",
        "\n",
        "filter_nsfw(nsfw_pretrained_pt, source_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Danger] Empty the output folder"
      ],
      "metadata": {
        "id": "nxAeOrV2gmh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "files = glob.glob(recognized_path + \"*\")\n",
        "for f in files:\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "vGwn1cSxgk18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IS5dRMWpilNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image recognizing\n"
      ],
      "metadata": {
        "id": "p_4TFWvbi51v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jN1BYIEZzx3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple"
      ],
      "metadata": {
        "id": "lyRsPouQyxUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See https://huggingface.co/erax-ai/EraX-NSFW-V1.0/blob/main/erax_nsfw_yolo11n.pt\n",
        "model = YOLO(\"pretrains/yolo11m.pt\")\n",
        "#model = YOLO(\"pretrains/erax_nsfw_yolo11m.pt\")\n",
        "results = model(model_path2, conf=CONFIDENCE_THRESHOLD, iou=IOU_THRESHOLD)\n",
        "json_sink = sv.JSONSink(recognized_path + \"report.json\")\n",
        "\n",
        "# results\n",
        "output = []\n",
        "\n",
        "# Save JSON\n",
        "with json_sink as sink:\n",
        "    # Cycle results\n",
        "    for result in results:\n",
        "        filename = os.path.basename(result.path)\n",
        "        recognized_filename = recognized_path + filename\n",
        "        annotated_image = result.orig_img.copy()\n",
        "        detected = []\n",
        "\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "        for found in detections.data.values():\n",
        "            detected = str(found)\n",
        "            detected_count = len(found)\n",
        "\n",
        "        custom_data = {\n",
        "            \"filename\": filename,\n",
        "            \"detections\": {\n",
        "                \"count\": detected_count,\n",
        "                \"items\": detected\n",
        "            }\n",
        "        }\n",
        "        output.append(custom_data)\n",
        "        sink.append(\n",
        "            detections,\n",
        "            custom_data\n",
        "        )\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TNGW7ZJEi9P1",
        "outputId": "27c00ab4-e993-4f23-838c-021621e47769"
      },
      "execution_count": 532,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x640 1 person, 1 cell phone, 1794.1ms\n",
            "1: 640x640 2 persons, 1 donut, 1 toothbrush, 1794.1ms\n",
            "Speed: 5.0ms preprocess, 1794.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "[{'filename': '1722985818194150.png', 'detections': {'count': 2, 'items': \"['person' 'cell phone']\"}}, {'filename': '1724897142625564.jpg', 'detections': {'count': 4, 'items': \"['person' 'person' 'toothbrush' 'donut']\"}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final"
      ],
      "metadata": {
        "id": "jyouKTZulrNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get image description function"
      ],
      "metadata": {
        "id": "OVNJE0FT5mJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moondream_model = md.vl(api_key=MOONDREAM_API_KEY)\n",
        "moondream_query = \"\"\"\n",
        "Give me a brief description of this image, and a detailed comma-separated list of unique objects visible in this image, no general terms.\n",
        "Output JSON object with keys:\n",
        "- 'subject': (string) An objective description of the subject in the image in 50 characters\n",
        "- 'short': (string) a short description for image label (max 25 words)\n",
        "- 'long': (string) a long description, max 250 characters\n",
        "- 'tags': (array) a list of univocal and not repeated 6 tags\n",
        "\"\"\"\n",
        "\n",
        "# Generate an AI image description\n",
        "## @params <string>                 img                   The image to process\n",
        "def get_image_description(img):\n",
        "    image = Image.open(img)\n",
        "    #encoded_image = moondream_model.encode_image(image)\n",
        "    description = {}\n",
        "\n",
        "    try:\n",
        "        answer = moondream_model.query(image, moondream_query)[\"answer\"]\n",
        "        description = json.loads(answer)\n",
        "    except json.JSONDecodeError as e:\n",
        "        #print(f\"Error decoding JSON: {e}, response: {answer}\")\n",
        "        description = {  # provide default values if JSON decode fails\n",
        "            \"subject\": None,\n",
        "            \"short\": None,\n",
        "            \"long\": None,\n",
        "            \"tags\": []\n",
        "        }\n",
        "    except Exception as e:\n",
        "        #print(f\"An unexpected error occurred: {e}\")\n",
        "        description = {  # provide default values if any other error occurs\n",
        "            \"subject\": None,\n",
        "            \"short\": None,\n",
        "            \"long\": None,\n",
        "            \"tags\": []\n",
        "        }\n",
        "\n",
        "    description = {\n",
        "        \"subject\": description.get(\"subject\"),\n",
        "        \"short\": description.get(\"short\"),\n",
        "        \"long\": description.get(\"long\"),\n",
        "        \"tags\": list(set(description.get(\"tags\")))\n",
        "    }\n",
        "    return description\n",
        "\n",
        "\n",
        "# Example in a cycle\n",
        "#for image in source_4_paths:\n",
        "#    print(get_image_description(image))"
      ],
      "metadata": {
        "id": "PhpEp2TcTGHA"
      },
      "execution_count": 534,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detect function"
      ],
      "metadata": {
        "id": "IddZNwjEiOAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a detection of a selected pretrained model on given source\n",
        "# The source can be both a string or array\n",
        "def detect(pretrained, source, sex_check = False):\n",
        "    detect = []\n",
        "    model = YOLO(pretrained)\n",
        "    results = model(source, conf=CONFIDENCE_THRESHOLD, iou=IOU_THRESHOLD)\n",
        "\n",
        "\n",
        "    for result in results:\n",
        "        file = os.path.basename(result.path)\n",
        "        recognized_filename = recognized_path + file\n",
        "        annotated_image = result.orig_img.copy()\n",
        "\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "        for found in detections.data.values():\n",
        "            detected = found.tolist()\n",
        "            detected_count = len(found)\n",
        "\n",
        "        if sex_check and detected_count > 0:\n",
        "            has_sex = True\n",
        "        else:\n",
        "            has_sex = False\n",
        "\n",
        "        custom_data = {\n",
        "            \"file\": file,\n",
        "            \"path\": recognized_path,\n",
        "            \"has_sex\": has_sex,\n",
        "            \"description\": get_image_description(result.path),\n",
        "            \"detections\": {\n",
        "                \"count\": detected_count,\n",
        "                \"items\": detected\n",
        "            }\n",
        "        }\n",
        "        detect.append(custom_data)\n",
        "    return detect"
      ],
      "metadata": {
        "id": "30_48f_KLege"
      },
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge detections function"
      ],
      "metadata": {
        "id": "TPLxT_rKiWe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subject_path = source_path\n",
        "\n",
        "default_detect = detect(yolo_pretrained_pt, subject_path)\n",
        "nsfw_detect = detect(nsfw_pretrained_pt, subject_path, True)\n",
        "\n",
        "def merge_detections(lst1, lst2):\n",
        "    merged_list = []  # Create a list to store merged results\n",
        "    for def_item, nsfw_item in zip(lst1, lst2):  # Assuming lst1 and lst2 have the same length and corresponding items\n",
        "        # Use square bracket notation to access dictionary keys\n",
        "        file = def_item.get(\"file\")\n",
        "        path = def_item.get(\"path\")\n",
        "\n",
        "        has_sex = def_item.get(\"has_sex\") or nsfw_item.get(\"has_sex\")\n",
        "        count = def_item.get(\"detections\", {}).get(\"count\", 0) + nsfw_item.get(\"detections\", {}).get(\"count\", 0)\n",
        "        items = def_item.get(\"detections\", {}).get(\"items\", []) + nsfw_item.get(\"detections\", {}).get(\"items\", [])\n",
        "        result = dict((i, items.count(i)) for i in items)\n",
        "\n",
        "        description = def_item.get(\"description\")\n",
        "\n",
        "        merged_item = {\n",
        "            \"file\": file,\n",
        "            \"path\": path,\n",
        "            \"has_sex\": has_sex,\n",
        "            \"description\": description,\n",
        "            \"detections\": {\n",
        "                \"count\": count,\n",
        "                \"items\": result\n",
        "            }\n",
        "        }\n",
        "        merged_list.append(merged_item)\n",
        "    return json.dumps(merged_list)\n",
        "\n",
        "print()\n",
        "print(\"final\", merge_detections(default_detect, nsfw_detect))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z4hdH_7zyPS",
        "outputId": "f82248a6-206e-4022-d460-480605979f04",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1378155513893.jpg: 640x640 (no detections), 2202.8ms\n",
            "image 2/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1651102464329.png: 640x384 (no detections), 1400.1ms\n",
            "image 3/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1698797738754547.jpg: 448x640 1 person, 17 cars, 1 truck, 1191.0ms\n",
            "image 4/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1701979302809986.jpg: 576x640 3 persons, 1223.4ms\n",
            "image 5/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1707539361936264.jpg: 640x448 1 person, 1 motorcycle, 965.9ms\n",
            "image 6/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1709881907307959.jpg: 640x512 1 kite, 1 clock, 1103.7ms\n",
            "image 7/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1713423666380556.jpg: 640x448 1 person, 986.1ms\n",
            "image 8/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1718904273290232.png: 416x640 1 person, 1 cat, 1 tie, 1 bed, 888.7ms\n",
            "image 9/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1719782258238061.jpg: 640x448 1 person, 962.9ms\n",
            "image 10/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1720141642525774.jpg: 640x480 5 persons, 1 motorcycle, 7 airplanes, 1035.6ms\n",
            "image 11/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1720645555420435.jpg: 640x512 1 person, 1 bed, 1102.7ms\n",
            "image 12/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721501914109498.jpg: 448x640 1 person, 1 bottle, 1 cup, 1 book, 1401.2ms\n",
            "image 13/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721668560245611.jpg: 640x512 1 person, 1773.0ms\n",
            "image 14/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721728036419286.jpg: 640x640 2 persons, 1 umbrella, 2202.4ms\n",
            "image 15/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721751848855669.jpg: 640x480 1 person, 1 bed, 1043.1ms\n",
            "image 16/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721834681148922.jpg: 640x480 1 person, 1 bed, 1050.6ms\n",
            "image 17/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1721985313295768.jpg: 640x448 3 persons, 1 bed, 1003.7ms\n",
            "image 18/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1722985818194150.png: 640x448 1 person, 1 cell phone, 979.7ms\n",
            "image 19/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723211294727707.jpg: 640x448 2 persons, 981.9ms\n",
            "image 20/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723399110043132.jpg: 640x640 1 person, 2 beds, 1 book, 1351.8ms\n",
            "image 21/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723519466751102.jpg: 384x640 2 persons, 1 couch, 1 bed, 1 toothbrush, 835.4ms\n",
            "image 22/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723652428084736.jpg: 448x640 1 person, 6 books, 952.0ms\n",
            "image 23/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1723903495615178.jpg: 640x544 2 persons, 1198.4ms\n",
            "image 24/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724419840794985.jpg: 448x640 1 person, 1 umbrella, 1527.3ms\n",
            "image 25/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724554844212123.jpg: 640x384 1 person, 4 bottles, 1 toilet, 1354.4ms\n",
            "image 26/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724579240484244.jpg: 448x640 2 persons, 2 beds, 1566.1ms\n",
            "image 27/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724642194903744.png: 480x640 1 person, 1 kite, 1197.6ms\n",
            "image 28/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724642947452792.png: 448x640 1 person, 4 kites, 956.2ms\n",
            "image 29/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724649057907120.png: 640x480 (no detections), 1027.5ms\n",
            "image 30/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724712668295029.png: 480x640 4 kites, 1017.6ms\n",
            "image 31/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724723747204379.jpg: 640x480 1 backpack, 1 toilet, 1 mouse, 3 remotes, 1026.9ms\n",
            "image 32/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724809972093412.jpg: 480x640 1 motorcycle, 1027.9ms\n",
            "image 33/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724897142625564.jpg: 640x512 2 persons, 1 toothbrush, 1119.0ms\n",
            "image 34/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1724905864533025.jpg: 640x640 2 persons, 2 chairs, 1358.6ms\n",
            "image 35/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1725093323532977s.jpg: 640x320 1 person, 1 vase, 708.8ms\n",
            "image 36/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726352613898890.jpg: 640x640 1 person, 2 beds, 1762.2ms\n",
            "image 37/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726477586651921.jpg: 640x544 1 person, 1 couch, 1 bed, 1845.0ms\n",
            "image 38/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726933948682880.jpg: 640x448 1 person, 1576.0ms\n",
            "image 39/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1726963008207714.jpg: 640x640 1 person, 1548.9ms\n",
            "image 40/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727142126122073.jpg: 608x640 1 couch, 1309.3ms\n",
            "image 41/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727200895869697.jpg: 448x640 1 person, 1 dining table, 1 tv, 1 cell phone, 1 toothbrush, 1018.5ms\n",
            "image 42/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727225350044664.png: 640x480 (no detections), 1101.2ms\n",
            "image 43/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727448034643355.jpg: 640x448 1 person, 1 motorcycle, 964.2ms\n",
            "image 44/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727971007198851.jpg: 640x448 1 person, 970.1ms\n",
            "image 45/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1727979723469241.jpg: 640x480 1 person, 1033.5ms\n",
            "image 46/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728061152737184.png: 448x640 2 persons, 960.9ms\n",
            "image 47/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728127236413072.jpg: 480x640 1 person, 1 skis, 1 bottle, 1 chair, 1015.4ms\n",
            "image 48/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728399968479679.jpg: 320x640 1 person, 1 dog, 1049.2ms\n",
            "image 49/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728480134770843.png: 640x544 1 person, 1 boat, 1 bed, 1881.9ms\n",
            "image 50/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1728557079613995.jpg: 640x384 1 person, 1 umbrella, 1 baseball bat, 1571.8ms\n",
            "image 51/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729021368547796.png: 384x640 1 person, 1 backpack, 1 cup, 1 chair, 1 couch, 1058.6ms\n",
            "image 52/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729345051888520.jpg: 640x384 2 persons, 1 handbag, 1 bottle, 1 sink, 858.4ms\n",
            "image 53/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729424628775413.jpg: 640x480 2 persons, 3 potted plants, 1 bed, 1026.8ms\n",
            "image 54/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729606389801732.jpg: 640x448 1 person, 1 cell phone, 970.6ms\n",
            "image 55/55 /content/drive/MyDrive/Colab Notebooks/training_samples/selected/1729868006558863.jpg: 640x512 1 kite, 1 potted plant, 1100.5ms\n",
            "Speed: 4.3ms preprocess, 1206.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        }
      ]
    }
  ]
}